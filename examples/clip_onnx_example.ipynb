{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip-onnx-example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHbV0vJ1uWPDaOYN4dW9TZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lednik7/CLIP-ONNX/blob/main/examples/clip_onnx_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart colab session after installation\n",
        "Reload the session if something doesn't work"
      ],
      "metadata": {
        "id": "fxPg_VvZuScV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "al_QNjyFq6Jj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/Lednik7/CLIP-ONNX.git\n",
        "!pip install onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget -c -O CLIP.png https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true"
      ],
      "metadata": {
        "id": "42eeJz9lTdJ6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuauIZIBSEUX",
        "outputId": "d5e6772f-4136-44a9-e6a1-9aa2652b35de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  6 12:48:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "print(onnxruntime.get_device())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqvxpdajRX5_",
        "outputId": "23453a4e-4947-48f2-e6c9-e6ecef09aa91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU inference mode"
      ],
      "metadata": {
        "id": "010k-ksVTjAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch CLIP"
      ],
      "metadata": {
        "id": "KdTz0IJWVBqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "# onnx cannot work with cuda\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
        "# batch first\n",
        "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).cpu() # [3, 77]"
      ],
      "metadata": {
        "id": "9ROPwKYurOhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cdd42a-3b00-48ce-99cf-9ad46e5ffb13"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 151MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit model(image, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CrHQ8cYt8Cx",
        "outputId": "c07b713e-7e88-419d-9913-28feb129dde8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 592 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP-ONNX"
      ],
      "metadata": {
        "id": "Ao2MriaVVG6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clip_onnx import clip_onnx, attention\n",
        "clip.model.ResidualAttentionBlock.attention = attention\n",
        "\n",
        "onnx_model = clip_onnx(model)\n",
        "onnx_model.convert2onnx(image, text, verbose=True)\n",
        "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode"
      ],
      "metadata": {
        "id": "nSeG9uAZrcph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82cd708a-6636-4f08-d472-d9241509e4bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLIP ONNX] Start convert visual model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/clip_onnx/utils.py:40: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  head_dim = q.shape[2] // num_heads\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:716: UserWarning: allowzero=0 by default. In order to honor zero value in shape use allowzero=1\n",
            "  warnings.warn(\"allowzero=0 by default. In order to honor zero value in shape use allowzero=1\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLIP ONNX] Start check visual model\n",
            "[CLIP ONNX] Start convert textual model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_opset9.py:2819: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  \"If indices include negative values, the exported graph will produce incorrect results.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLIP ONNX] Start check textual model\n",
            "[CLIP ONNX] Models converts succesfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit onnx_model(image, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B15dr51UrvMh",
        "outputId": "4634f026-efe3-4622-de6d-de9069a27844"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 568 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU inference mode\n",
        "Select a runtime GPU to continue:\n",
        "\n",
        "Click Runtime -> Change Runtime Type -> switch \"Harware accelerator\" to be GPU. Save it, and you maybe connect to GPU"
      ],
      "metadata": {
        "id": "Ahh_7CkTUb8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP-ONNX"
      ],
      "metadata": {
        "id": "B6M7yq7qceb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model.start_sessions(providers=[\"CUDAExecutionProvider\"]) # GPU mode"
      ],
      "metadata": {
        "id": "6LtPSZhfUd_m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit onnx_model(image, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPUVzqmgcYas",
        "outputId": "752922cd-accf-4afa-897f-978a5cdf2068"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 193.20 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 62.2 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch CLIP"
      ],
      "metadata": {
        "id": "jb58mrkbch2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\"\n",
        "# onnx cannot work with cuda\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "# batch first\n",
        "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device) # [1, 3, 224, 224]\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device) # [3, 77]"
      ],
      "metadata": {
        "id": "gidR99GOckyF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit model(image, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpBrtjlOcwOC",
        "outputId": "f2ec71ef-3f47-4edb-9dfd-3b792f20f2ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 89.8 ms per loop\n"
          ]
        }
      ]
    }
  ]
}